{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fsolve, fminbound, fmin_bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quasi-Newton unconstained optimization \n",
    "def quasi_newton_opt(x0, func, func_grad, bounds, max_iter=50, full_output=False):\n",
    "    # Function to perform a quasi-Newton optimization\n",
    "    # for an unconstrained problem with BFGS Hessian update\n",
    "\n",
    "    # x0: Inital point\n",
    "    # func: objective function\n",
    "    # func_grad: Gradient of function\n",
    "    # bounds: design variable bounds\n",
    "    # max_iter: maximum number of iterations \n",
    "    # full_output: option to enable full output\n",
    "\n",
    "    B0 = np.identity(len(x0))    # initializing hessian\n",
    "    iter_no = 0\n",
    "    while True:\n",
    "        delta_f0 = func_grad(x0, func)    # calculating gradient\n",
    "        sq = np.dot(-B0, delta_f0)  # calculating search direction\n",
    "        sq = sq / np.sqrt(np.sum(sq**2))    # normalizing search direction\n",
    "\n",
    "        alpha_max = [0]\n",
    "        for i in range(len(sq)):\n",
    "            if sq[i] > 0:\n",
    "                alpha_max = max(alpha_max, (bounds[i][1] - x0[i])/sq[i])\n",
    "            else:\n",
    "                alpha_max = max(alpha_max, (bounds[i][0] - x0[i])/sq[i])\n",
    "\n",
    "        alpha = fminbound(lambda alpha: func(x0 + alpha*sq), 0, alpha_max[0])  # line search along sq\n",
    "        x_new = x0 + alpha * sq # new point\n",
    "        delta_f_new = func_grad(x_new, func)  # gradient at new point \n",
    "        delta_x = x_new - x0    # change in x\n",
    "        delta_delta = delta_f_new - delta_f0    # change in gradient\n",
    "\n",
    "        # Checking for stationary point or max iterations:\n",
    "        if max(abs(delta_f_new)) < 1E-06:\n",
    "            exit_condition = 'Gradient less than tolarence'\n",
    "            break\n",
    "        elif max(abs(delta_x)) < 1E-06:\n",
    "            exit_condition = 'Design point update less than tolarence'\n",
    "            break\n",
    "        elif iter_no >= max_iter:\n",
    "            exit_condition = 'Max iteration reached'\n",
    "            break\n",
    "\n",
    "        \n",
    "        delta_B = BFGS_update(B0, delta_x, delta_delta) # BFGS hessian update\n",
    "\n",
    "        # Updating values for next iteration\n",
    "        B_new = B0 + delta_B\n",
    "        B0 = B_new\n",
    "        x0 = x_new\n",
    "        delta_f0 = delta_f_new\n",
    "        iter_no = iter_no + 1\n",
    "\n",
    "    # Output\n",
    "    if full_output:\n",
    "        return [x_new, delta_f_new, B_new, iter_no, exit_condition]\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained augmented lagrangian optimization\n",
    "def augmented_lagrangian(x0, func, func_grad, con, bounds, rho0=1, max_iter=50, full_output=False):\n",
    "    # Function to perform a augmented Lagrangian optimization\n",
    "    # for an unconstrained problem with quasi-Newton unconstrained optimization\n",
    "\n",
    "    # x0: Inital point\n",
    "    # func: objective function\n",
    "    # func_grad: Gradient of function\n",
    "    # con: constraint functions\n",
    "    # bounds: design variable bounds\n",
    "    # rho: penalty value\n",
    "    # max_iter: maximum number of iterations \n",
    "    # full_output: option to enable full output\n",
    "\n",
    "    g, h = con(x0)  # constraints at inital point\n",
    "    m0 = np.zeros((len(g), 1))  # initializing mu\n",
    "    l0 = np.zeros((len(h), 1))  # initializing lambda\n",
    "    m = np.array(m0)    # mu array\n",
    "    l = np.array(l0)    # lambda array\n",
    "\n",
    "    # Augmented Lagrangian loop\n",
    "    iter_no = 0 \n",
    "    while True:\n",
    "        x = quasi_newton_opt(x0, lambda x: calc_aug_lagrangian(x, func, con, m0, l0, rho0), func_grad, bounds)  # performing quasi-Newton opt. on augmented Lagrangian\n",
    "\n",
    "        g, h = con(x)   # constraint value at new design point\n",
    "        m = m0 + rho0*g # updating mu\n",
    "        for i in range(len(m0)):\n",
    "            m[i] = max(0, m[i]) \n",
    "        l = l0 + rho0*h # updating lambda\n",
    "\n",
    "        delta_L = func_grad(x, lambda x: calc_lagrangian(x, func, con, m, l))   # gradient of Lagrangian at new design point\n",
    "        delta_x = x - x0    # change in design point\n",
    "\n",
    "        # Checking for stationary point or max iterations:\n",
    "        if max(abs(delta_L)) < 1E-06:\n",
    "            exit_condition = 'Gradient less than tolarence'\n",
    "            break\n",
    "        elif max(abs(delta_x)) < 1E-06:\n",
    "            exit_condition = 'Design point update less than tolarence'\n",
    "            break\n",
    "        elif iter_no >= max_iter:\n",
    "            exit_condition = 'Max iteration reached'\n",
    "            break\n",
    "\n",
    "        # Updating variables for next iteration\n",
    "        x0 = x\n",
    "        m0 = m\n",
    "        l0 = l\n",
    "        iter_no = iter_no + 1\n",
    "\n",
    "    # Output    \n",
    "    if full_output:\n",
    "        return (x, delta_L, m, l, g, h, iter_no, exit_condition)\n",
    "    return x\n",
    "\n",
    "def calc_lagrangian(x, func, con, m, l):\n",
    "    # Function to calculate Lagrangian \n",
    "\n",
    "    # x: Evaluation point\n",
    "    # func: objective function\n",
    "    # con: constraint function\n",
    "    # m: mu\n",
    "    # l: lambda\n",
    "\n",
    "    g, h = con(x)   # calculating constraints at evaluation point\n",
    "\n",
    "    # Calculating Lagrangian\n",
    "    if h.size and g.size:   # both h and g present\n",
    "        return func(x) + np.dot(m.transpose(), g) + np.dot(l.transpose(), h)\n",
    "    elif h.size:    # only h present\n",
    "        return func(x) + np.dot(l.transpose(), h)\n",
    "    else:   # only g present\n",
    "        return func(x) + np.dot(m.transpose(), g)\n",
    "\n",
    "\n",
    "\n",
    "def calc_aug_lagrangian(x, func, con, m, l, rho):\n",
    "        # Function to calculate Lagrangian \n",
    "\n",
    "    # x: Evaluation point\n",
    "    # func: objective function\n",
    "    # con: constraint function\n",
    "    # m: mu\n",
    "    # l: lambda\n",
    "    # rho: quadratic penalty\n",
    "\n",
    "    g, h = con(x)   # calculating constraints at evaluation point\n",
    "\n",
    "    # Calculating augmented Lagrangian\n",
    "    if h.size and g.size:   # both h and g present\n",
    "        return func(x) + np.dot(m.transpose(), g) + np.dot(l.transpose(), h) + 0.5 * rho * ((np.dot(g.transpose(), g))**2 + (np.dot(h.transpose(), h))**2)\n",
    "    elif h.size:    # only h present\n",
    "        return func(x) + np.dot(l.transpose(), h) + 0.5 * rho * ((np.dot(h.transpose(), h))**2)\n",
    "    else:   # only g present\n",
    "        return func(x) + np.dot(m.transpose(), g) + 0.5 * rho * ((np.dot(g.transpose(), g))**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    # Objective function definition\n",
    "\n",
    "    # x: Evaluation point\n",
    "\n",
    "    f = 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2 \n",
    "    return f\n",
    "\n",
    "def con(x):\n",
    "    # Constraint function definition\n",
    "\n",
    "    # x: Evaluation point\n",
    "\n",
    "    g = np.array([x[0]**2 + x[1]**2 - 0.5])\n",
    "    h = np.array([])\n",
    "    return (g, h)\n",
    "\n",
    "\n",
    "def penalty(x, p=100):\n",
    "    # Penalty function to convert constrained to unconstrained problem \n",
    "    \n",
    "    # x: Evaluation point\n",
    "    # p: quadratic penalty\n",
    "\n",
    "    con_sum = 0 # initializing total penalty \n",
    "    g, h = con(x)   # calculating constraints at evaluation point\n",
    "\n",
    "    # Calculating penalty\n",
    "    for gi in g:\n",
    "        con_sum = con_sum + max(0, gi)\n",
    "    for hi in h:\n",
    "        con_sum = con_sum + max(0, hi)\n",
    "    return func(x) + p * (con_sum)**2\n",
    "\n",
    "\n",
    "def func_grad(x, func):\n",
    "    # Function to calculate gradient \n",
    "    # of objective function using forward FD\n",
    "\n",
    "    # x: Evaluation point\n",
    "\n",
    "    step = 1E-8 # step size for FD\n",
    "    delta_f = np.zeros((len(x), 1)) # initializing gradient\n",
    "    for i in range(len(x)):\n",
    "        x_temp = np.array(x, dtype=float)\n",
    "        x_temp[i] = x[i] + step\n",
    "        delta_f[i] = (func(x_temp) - func(x)) / step    # calculating gradient using forward FD\n",
    "    return delta_f\n",
    "\n",
    "\n",
    "\n",
    "def BFGS_update(B, delta_x, delta_delta):\n",
    "    # Function to update to hessian\n",
    "    # using BFGS method\n",
    "\n",
    "    # B: initial hessian\n",
    "    # delta_x: change in evaluation point\n",
    "    # delta_delta: change in gradient\n",
    "    \n",
    "    delta_x_T = delta_x.transpose()\n",
    "    delta_delta_T = delta_delta.transpose()\n",
    "    delta_B = (1 + (np.dot(np.dot(delta_delta_T, B), delta_delta))/(np.dot(delta_x_T, delta_delta))) * \\\n",
    "    (np.dot(delta_x, delta_x_T)/np.dot(delta_x_T, delta_delta)) - \\\n",
    "        (np.dot(delta_x, np.dot(delta_delta_T, B)) + np.dot(np.dot(delta_delta_T, B).transpose(), delta_x_T)) / \\\n",
    "            (np.dot(delta_x_T, delta_delta))\n",
    "    \n",
    "    return delta_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x, x_ref):\n",
    "    # Function to scale design variables to order of 1\n",
    "\n",
    "    # x: unscaled design variables\n",
    "    # x_ref: reference motor values\n",
    "\n",
    "    return x / x_ref\n",
    "\n",
    "def descale(x_scaled, x_ref):\n",
    "    # Function to descale design variables\n",
    "\n",
    "    # x_scaled: scaled design variables\n",
    "    # x_ref: reference motor values\n",
    "\n",
    "    return x_scaled * x_ref\n",
    "\n",
    "def calc_obj_func(x_scaled):\n",
    "    # Function to calculate the objective function \n",
    "\n",
    "    # x: unscaled design variables\n",
    "    # x_ref: reference motor values\n",
    "\n",
    "    import constants as c   # importing constants\n",
    "    x = descale(x_scaled, c.x_ref)  # descaling the design valiables\n",
    "    P_c, A_t, A_e = x\n",
    "    t_cyl = (P_c + 10E+05) * c.R_tank * c.f / c.sigma_tank  # thickness of cylindrical tank sections [m]\n",
    "    t_sph = t_cyl / 2   # thickness of spherical tank sections [m]\n",
    "\n",
    "    mass_UDMH_tank = 1.2 * (2*t_cyl*np.pi*c.R_tank*c.L_UDMH_tank + 4*t_sph*c.R_tank**2) * c.rho_tank    # mass of UDMH tank [kg]\n",
    "    mass_N2O4_tank = 1.2 * (2*t_cyl*np.pi*c.R_tank*c.L_N2O4_tank + 4*t_sph*c.R_tank**2) * c.rho_tank    # mass of N2O4 tank [kg]\n",
    "\n",
    "    mass_flow = c.Gamma * P_c * A_t / np.sqrt(c.R * c.T_c)  # mass flow through the nozzle [kg/s]\n",
    "    A_c = (mass_flow * c.R * c.T_c) / (0.3 * P_c * np.sqrt(c.gamma * c.R * c.T_c))  # chamber exit area [m2]\n",
    "    R_c = np.sqrt(A_c / np.pi)  # chamber radius [m]\n",
    "    k_loads = 1 # correction for high chamber pressure\n",
    "    V_c = np.pi * R_c**2 * c.L_c    # chamber volume [m3]\n",
    "    chamber_mass = k_loads * (2/(c.L_c/R_c) + 2) * c.rho/c.sigma * c.f * P_c * V_c  # chamber mass [kg]\n",
    "    injector_mass = c.rho/c.sigma * c.f * (1.2 * A_c * R_c * np.sqrt(P_c*c.sigma))  # injector plate mass [kg]\n",
    "\n",
    "    # nozzle_convergent_mass = (rho/sigma) * f * (A_t * (((A_c/A_t)-1)/sind(beta)) * (P_c*R_c));  % convergent nozzle mass using thin shell theory (kg)\n",
    "    nozzle_divergent_mass = (c.rho/c.sigma) * c.f * (A_t * (((A_e/A_t)-1)/np.sin(c.alpha)) * (P_c*R_c)) # nozzle mass [kg]\n",
    "\n",
    "    total_mass = chamber_mass + injector_mass + nozzle_divergent_mass + mass_UDMH_tank + mass_N2O4_tank # total motor mass [kg]\n",
    "    \n",
    "    return total_mass/c.mass_dry_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optima: \n",
      "[[0.6063714 ]\n",
      " [0.36631198]]\n",
      "Gradient at optima: \n",
      "[[-0.00017675]\n",
      " [-0.00074708]]\n",
      "Hessian inverse at optima: \n",
      "[[0.00172857 0.00036459]\n",
      " [0.00036459 0.00332228]]\n",
      "Iterations: 50\n",
      "Max iteration reached\n"
     ]
    }
   ],
   "source": [
    "# Test penalty optimization\n",
    "\n",
    "x0 = np.array([[0, 0]]).transpose()\n",
    "bounds = np.array([[-5,-5], [5, 5]], dtype=float).transpose()\n",
    "[x, delta_f, B, iter_no, exit_condition] = quasi_newton_opt(x0, penalty, func_grad, bounds, max_iter=50, full_output=True)\n",
    "print(f'Optima: \\n{x}\\nGradient at optima: \\n{delta_f}\\nHessian inverse at optima: \\n{B}')\n",
    "print(f'Iterations: {iter_no}')\n",
    "print(exit_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.155482\n",
      "         Iterations: 15\n",
      "         Function evaluations: 90\n",
      "         Gradient evaluations: 30\n",
      "[0.60637197 0.36631452]\n",
      "(array([0.0018733]), array([], dtype=float64))\n"
     ]
    }
   ],
   "source": [
    "# Test penalty optimization with inbuilt unconstrainted optimizer\n",
    "\n",
    "x0 = np.array([0, 0], dtype=float)\n",
    "x = fmin_bfgs(penalty, x0)\n",
    "print(x)\n",
    "print(con(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optima: \n",
      "[[0.60548143]\n",
      " [0.3652325 ]]\n",
      "Gradient of L at optima: \n",
      "[[ 1.41553436e-05]\n",
      " [-3.35287353e-06]]\n",
      "mu: \n",
      "[[0.37653719]]\n",
      "lambda: \n",
      "[]\n",
      "g: \n",
      "[[2.53620682e-06]]\n",
      "h: \n",
      "[]\n",
      "\n",
      "Iterations: 6\n",
      "Design point update less than tolarence\n"
     ]
    }
   ],
   "source": [
    "# Test augmented Lagrangian optimization\n",
    "\n",
    "x0 = np.array([[0, 0]]).transpose()\n",
    "bounds = np.array([[-5,-5], [5, 5]], dtype=float).transpose()\n",
    "x, delta_L, m, l, g, h, iter_no, exit_condition = augmented_lagrangian(x0, func, func_grad, con, bounds, rho0=1.1, full_output=True)\n",
    "print(f'Optima: \\n{x}\\nGradient of L at optima: \\n{delta_L}\\nmu: \\n{m}\\nlambda: \\n{l}\\ng: \\n{g}\\nh: \\n{h}\\n')\n",
    "print(f'Iterations: {iter_no}')\n",
    "print(exit_condition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bf06b8021c62b15361c0f1ff6f7b4a98d4739984d2745f50473cee6c668c345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
