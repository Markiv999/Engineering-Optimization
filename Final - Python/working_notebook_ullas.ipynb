{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fsolve, fminbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quasi-Newton unconstained optimization \n",
    "def quasi_newton_opt(x0, func_grad, func_line_search, max_iter=500, full_output=False):\n",
    "    # Function to perform a quasi-Newton optimization\n",
    "    # for an unconstrained problem with BFGS Hessian update\n",
    "\n",
    "    # x0: Inital point\n",
    "    # func_grad: Gradient of function\n",
    "    # func_line_search: Objective function with used for line search\n",
    "\n",
    "    B0 = np.identity(2, dtype=float)    # initializing hessian\n",
    "    iter_no = 0\n",
    "    stop = False\n",
    "    while True:\n",
    "        delta_f0 = func_grad(x0)    # calculating gradient\n",
    "        sq = np.dot(-B0, delta_f0)  # calculating search direction\n",
    "        alpha = fminbound(lambda alpha: func_line_search(x0, alpha, sq), 0, 2)  # line search along sq\n",
    "        x_new = x0 + alpha * sq # new point\n",
    "        delta_f_new = func_grad(x_new)  # gradient at new point \n",
    "\n",
    "        # Checking for stationary point:\n",
    "        for delta_f in delta_f_new:\n",
    "            if abs(delta_f) > 1E-07:\n",
    "                stop = False\n",
    "            else:\n",
    "                stop = True\n",
    "        if stop or iter_no > max_iter:\n",
    "            break\n",
    "\n",
    "        delta_x = x_new - x0    # change in x\n",
    "        delta_delta = delta_f_new - delta_f0    # change in gradient\n",
    "        delta_B = BFGS_update(B0, delta_x, delta_delta) # BFGS hessian update\n",
    "\n",
    "        # Updating values for next iteration\n",
    "        B_new = B0 + delta_B\n",
    "        B0 = B_new\n",
    "        x0 = x_new\n",
    "        delta_f0 = delta_f_new\n",
    "        iter_no = iter_no + 1\n",
    "        \n",
    "    if full_output:\n",
    "        return [x_new, delta_f_new, B_new, iter_no]\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    # Objective function definition\n",
    "\n",
    "    # x: Evaluation point\n",
    "    f = (x[0] + 2*x[1] - 7)**2 + (2*x[0] + x[1] - 5)**2 # Test quadratic function\n",
    "    return f\n",
    "\n",
    "\n",
    "def func_grad(x):\n",
    "    # Function to calculate gradient \n",
    "    # of objective function using forward FD\n",
    "\n",
    "    # x: Evaluation point\n",
    "\n",
    "    step = 1E-8 # step size for FD\n",
    "    delta_f = np.zeros((2, 1))\n",
    "    delta_f[0] = (func(np.array([x[0]+step, x[1]])) - func(x)) / step\n",
    "    delta_f[1] = (func(np.array([x[0], x[1]+step])) - func(x)) / step\n",
    "    return delta_f\n",
    "\n",
    "# def func_grad_test(x):\n",
    "#     step = 1E-8\n",
    "#     x_dim = len(x)\n",
    "#     delta_f = np.zeros((2, 1))\n",
    "#     for i in range(x_dim):\n",
    "#         x_new = np.array(x)\n",
    "#         x_new[i] = x[i] + step\n",
    "#         delta_f[i] = (func(x_new) - func(x)) / step\n",
    "#     return delta_f\n",
    "\n",
    "\n",
    "def func_line_search(x0, alpha, sq):\n",
    "    # Function for line search\n",
    "\n",
    "    # x0: Initial point\n",
    "    # alpha: Scaling parameter\n",
    "    # sq: Search direction\n",
    "    \n",
    "    x_new = x0 + alpha*sq\n",
    "    return func(x_new)\n",
    "\n",
    "\n",
    "def BFGS_update(B, delta_x, delta_delta):\n",
    "    # Function to update to hessian\n",
    "    # using BFGS method\n",
    "\n",
    "    # B: initial hessian\n",
    "    # delta_x: change in evaluation point\n",
    "    # delta_delta: change in gradient\n",
    "    \n",
    "    delta_x_T = delta_x.transpose()\n",
    "    delta_delta_T = delta_delta.transpose()\n",
    "    delta_B = (1 + (np.dot(np.dot(delta_delta_T, B), delta_delta))/(np.dot(delta_x_T, delta_delta))) * \\\n",
    "    (np.dot(delta_x, delta_x_T)/np.dot(delta_x_T, delta_delta)) - \\\n",
    "        (np.dot(delta_x, np.dot(delta_delta_T, B)) + np.dot(np.dot(delta_delta_T, B).transpose(), delta_x_T)) / \\\n",
    "            (np.dot(delta_x_T, delta_delta))\n",
    "    \n",
    "    return delta_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optima: \n",
      "[[1.]\n",
      " [3.]]\n",
      "Gradient at optima: \n",
      "[[4.97392675e-08]\n",
      " [5.02383101e-08]]\n",
      "Hessian inverse at optima: \n",
      "[[ 0.27777779 -0.22222223]\n",
      " [-0.22222223  0.27777779]]\n",
      "Iterations: 2\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([[0, 0]]).transpose()\n",
    "[x, delta_f, B, iter_no] = quasi_newton_opt(x0, func_grad, func_line_search, full_output=True)\n",
    "print(f'Optima: \\n{x}\\nGradient at optima: \\n{delta_f}\\nHessian inverse at optima: \\n{B}')\n",
    "print(f'Iterations: {iter_no}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82a59c0bad32f961f86610a9c6f7d5bee0daa017c0134de324e8d46a81872590"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
