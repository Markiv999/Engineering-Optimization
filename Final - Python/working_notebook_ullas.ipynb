{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries for optimization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(A, b):\n",
    "    '''\n",
    "    Solves the least squares problem Ax = b\n",
    "    Args:\n",
    "        A: co-efficients matrix\n",
    "        b: constants vector\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "    '''\n",
    "\n",
    "    return np.linalg.inv(A.T @ A) @ A.T @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite difference methods:\n",
    "def forward_diff(f, x, h=1E-08):\n",
    "    '''\n",
    "    Solves the forward difference problem\n",
    "    Args:\n",
    "        f: function\n",
    "        x: vector of variables\n",
    "        h: step size\n",
    "    Returns:\n",
    "        delta_f: solution vector\n",
    "    '''\n",
    "\n",
    "    delta_f = np.zeros(x.shape) # Initialize delta_f\n",
    "    for i in range(x.shape[0]):\n",
    "        x_forw = np.array(x) # Make a copy of x\n",
    "        x_forw[i] += h   # Increment x_forw[i] by h\n",
    "        delta_f[i] = (f(x_forw) - f(x)) / h # Calculate the forward difference\n",
    "\n",
    "    return delta_f\n",
    "\n",
    "\n",
    "def central_diff(f, x, h=1E-08):\n",
    "    '''\n",
    "    Solves the central difference problem\n",
    "    Args:\n",
    "        f: function\n",
    "        x: vector of variables\n",
    "        h: step size\n",
    "    Returns:\n",
    "        delta_f: solution vector\n",
    "    '''\n",
    "\n",
    "    delta_f = np.zeros(x.shape) # Initialize delta_f\n",
    "    for i in range(x.shape[0]):\n",
    "        x_forw = np.array(x) # Make a copy of x\n",
    "        x_back = np.array(x) # Make a copy of x\n",
    "        x_forw[i] += h   # Increment x_forw[i] by h\n",
    "        x_back[i] -= h   # Decrement x_back[i] by h\n",
    "        delta_f [i]= (f(x_forw) - f(x_back)) / (2*h) # Calculate the central difference\n",
    "\n",
    "    return delta_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting constrained optimization problem to unconstrained optimization problem:\n",
    "# Using penalty method:\n",
    "def constrained_to_unconstrained(f, con, x, p=100):\n",
    "    '''\n",
    "    Converts a constrained optimization problem to an unconstrained optimization problem\n",
    "    Args:\n",
    "        f: function\n",
    "        con: constraint function\n",
    "        x: vector of variables\n",
    "        p: penalty parameter\n",
    "    Returns:\n",
    "        f_uncon: unconstrained function\n",
    "    '''\n",
    "    \n",
    "    # Objective function\n",
    "    def f_unconstrained(x):\n",
    "        g, h = con(x) # Calculate the constraint function\n",
    "        con_sum = 0 # Initialize the constraint sum\n",
    "        for i in range(g.shape[0]):\n",
    "            con_sum += max(0, g[i])**2 # Add the constraint sum\n",
    "        for i in range(h.shape[0]):\n",
    "            con_sum += h[i]**2  # Add the constraint sum\n",
    "        return f(x) + p*con_sum # Return the unconstrained objective function\n",
    "        \n",
    "    return f_unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unconstrained optimization:\n",
    "# 1st order methods:\n",
    "\n",
    "def steepest_descent(f, x, delta_f, grad_tol=1E-05, delta_f_tol=1E-05, delta_x_tol=1E-05, max_iter=100, full_output=False):\n",
    "    '''\n",
    "    Optimizes the unconstrained problem using the steepest descent method\n",
    "    Args:\n",
    "        f: objective function\n",
    "        x: initial value of vector of variables\n",
    "        delta_f: gradient of the objective function\n",
    "        grad_tol: gradient tolerance\n",
    "        delta_f_tol: objective function update tolerance\n",
    "        delta_x_tol: design variable update tolerance\n",
    "        max_iter: maximum number of iterations\n",
    "        full_output: whether to return the full output or not\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "        f_val: objective function value\n",
    "        exit_flag: exit flag\n",
    "        iter: number of iterations\n",
    "    '''\n",
    "\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "    while not convergence:\n",
    "        s = -delta_f(f, x) # Calculate the search direction\n",
    "        s = s / np.linalg.norm(s) # Normalize the search direction\n",
    "        alpha = opt.fminbound(lambda alpha: f(x + alpha*s), 0, 1) # Calculate the step size\n",
    "        x_new = x + alpha*s # Calculate the new x\n",
    "        iter += 1 # Increment the iteration counter\n",
    "        \n",
    "        # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.linalg.norm(delta_f(f, x_new)) <= grad_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.linalg.norm(x_new - x) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        elif np.linalg.norm(f(x_new) - f(x)) <= delta_f_tol:    # Check if objective function update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 3\n",
    "            \n",
    "        x = x_new # Update x\n",
    "\n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    else:\n",
    "        return x_new\n",
    "\n",
    "\n",
    "def conjugate_gradient(f, x, delta_f, grad_tol=1E-05, delta_f_tol=1E-05, delta_x_tol=1E-05, max_iter=100, full_output=False):\n",
    "    '''\n",
    "    Optimizes the unconstrained problem using the conjugate gradient method\n",
    "    Args:\n",
    "        f: objective function\n",
    "        x: initial value of vector of variables\n",
    "        delta_f: gradient of the objective function\n",
    "        grad_tol: gradient tolerance\n",
    "        delta_f_tol: objective function update tolerance\n",
    "        delta_x_tol: design variable update tolerance\n",
    "        max_iter: maximum number of iterations\n",
    "        full_output: whether to return the full output or not\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "        f_val: objective function value\n",
    "        exit_flag: exit flag\n",
    "        iter: number of iterations\n",
    "    '''\n",
    "\n",
    "    s = -delta_f(f, x) # Calculate the initial search direction\n",
    "    s = s / np.linalg.norm(s) # Normalize the initial search direction\n",
    "    alpha = opt.fminbound(lambda alpha: f(x + alpha*s), 0, 1) # Calculate the initial step size\n",
    "    x_new = x + alpha*s # Calculate the initial x update\n",
    "\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "    while not convergence:\n",
    "        if iter % x.shape[0] == 0:  # Check if the iteration counter is a multiple of the number of design variables\n",
    "            s = -delta_f(f, x_new) # Calculate the search direction\n",
    "        else:\n",
    "            s = -delta_f(f, x_new) + (np.linalg.norm(delta_f(f, x_new))**2 / np.linalg.norm(delta_f(f, x))**2) * s # Calculate the search direction\n",
    "\n",
    "        s = s / np.linalg.norm(s) # Normalize the search direction\n",
    "        alpha = opt.fminbound(lambda alpha: f(x_new + alpha*s), 0, 1) # Calculate the step size\n",
    "        x = x_new # Update x\n",
    "        x_new = x + alpha*s # Calculate the new x\n",
    "        iter += 1 # Increment the iteration counter\n",
    "\n",
    "        # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.linalg.norm(delta_f(f, x_new)) <= grad_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.linalg.norm(x_new - x) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        elif np.linalg.norm(f(x_new) - f(x)) <= delta_f_tol:    # Check if objective function update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 3\n",
    "        \n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    else:\n",
    "        return x_new\n",
    "\n",
    "\n",
    "# Hessian update rules:\n",
    "def BFGS_update(B, delta_x, delta_delta):\n",
    "    '''\n",
    "    Calculates the BFGS update matrix\n",
    "    Args:\n",
    "        B: initial Hessian estimate\n",
    "        delta_x: design variable update\n",
    "        delta_delta: objective function update\n",
    "    Returns:\n",
    "        deta_B: update to the Hessian estimate\n",
    "    '''\n",
    "\n",
    "    return (1 + delta_delta.T@B@delta_delta / (delta_delta.T@delta_x)) * (delta_x@delta_x.T) / (delta_x.T@delta_delta) - \\\n",
    "        (delta_x@(delta_delta.T@B) + (delta_delta.T@B).T@delta_x.T) / (delta_x.T@delta_delta)\n",
    "\n",
    "def DFP_update(B, delta_x, delta_delta):\n",
    "    '''\n",
    "    Calculates the DFP update matrix\n",
    "    Args:\n",
    "        B: initial Hessian estimate\n",
    "        delta_x: design variable update\n",
    "        delta_delta: objective function update\n",
    "    Returns:\n",
    "        deta_B: update to the Hessian estimate\n",
    "    '''\n",
    "\n",
    "    return delta_x@delta_x.T / (delta_x.T@delta_delta) - \\\n",
    "        (B@delta_delta)@(B@delta_delta).T / (delta_delta.T@B@delta_delta)\n",
    "\n",
    "\n",
    "# Quasi-Newton method:\n",
    "def quasi_newton(f, x, delta_f, grad_tol=1E-05, delta_f_tol=1E-05, delta_x_tol=1E-05, max_iter=100, full_output=False, update_rule=BFGS_update):\n",
    "    '''\n",
    "    Optimizes the unconstrained problem using the quasi-Newton method\n",
    "    Args:\n",
    "        f: objective function\n",
    "        x: initial value of vector of variables\n",
    "        delta_f: gradient of the objective function\n",
    "        grad_tol: gradient tolerance\n",
    "        delta_f_tol: objective function update tolerance\n",
    "        delta_x_tol: design variable update tolerance\n",
    "        max_iter: maximum number of iterations\n",
    "        full_output: whether to return the full output or not\n",
    "        update_rule: update rule for the Hessian estimate\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "        f_val: objective function value\n",
    "        exit_flag: exit flag\n",
    "        iter: number of iterations\n",
    "    '''\n",
    "\n",
    "    B = np.eye(x.shape[0]) # Initialize the Hessian approximation\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "\n",
    "    while not convergence:\n",
    "        s = -B@delta_f(f, x)   # Calculate the search direction\n",
    "        s = s / np.linalg.norm(s) # Normalize the search direction\n",
    "        alpha = opt.fminbound(lambda alpha: f(x + alpha*s), 0, 1) # Calculate the step size\n",
    "        x_new = x + alpha*s # Calculate the new x\n",
    "        iter += 1 # Increment the iteration counter\n",
    "\n",
    "         # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.linalg.norm(delta_f(f, x_new)) <= grad_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.linalg.norm(x_new - x) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        elif np.linalg.norm(f(x_new) - f(x)) <= delta_f_tol:    # Check if objective function update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 3\n",
    "\n",
    "        B = B + update_rule(B, x_new - x, delta_f(f, x_new) - delta_f(f, x)) # Update the Hessian approximation\n",
    "        x = x_new # Update x\n",
    "\n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    else:\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained optimization methods:\n",
    "\n",
    "def calc_lagrangian(f, con, x, lambda_, mu):\n",
    "    '''\n",
    "    Calculates the lagrangian of the function\n",
    "    Args:\n",
    "        f: objective function\n",
    "        con: constraint function\n",
    "        x: vector of variables\n",
    "        lambda_: Lagrange equality multipliers\n",
    "        mu: Lagrange inequality multipliers\n",
    "    Returns:\n",
    "        lagrangian: lagrangian of the function\n",
    "    '''\n",
    "    g, h = con(x)   # Calculate the inequality and equality constraints\n",
    "    \n",
    "    return f(x) + lambda_@h + mu@g   # Calculate the lagrangian\n",
    "\n",
    "\n",
    "# Augmented Lagrangian method:\n",
    "def calc_augmented_lagrangian(f, con, x, lambda_, mu, rho):\n",
    "    '''\n",
    "    Calculates the augmented Lagrangian of the function\n",
    "    Args:\n",
    "        f: objective function\n",
    "        con: constraint function\n",
    "        x: vector of variables\n",
    "        lambda_: Lagrange equality multipliers\n",
    "        mu: Lagrange inequality multipliers\n",
    "        rho: augmented Lagrangian penalty parameter\n",
    "    Returns:\n",
    "        augmented_lagrangian: augmented Lagrangian of the function\n",
    "    '''\n",
    "\n",
    "    g, h = con(x) # Calculate the inequality and equality constraints\n",
    "    con_sum = 0\n",
    "    for i in range(g.shape[0]):\n",
    "        con_sum += max(0, g[i])**2\n",
    "    for i in range(h.shape[0]):\n",
    "        con_sum += h[i]**2\n",
    "    \n",
    "    return calc_lagrangian(f, con, x, lambda_, mu) +  rho * con_sum # Calculate the augmented Lagrangian function\n",
    "\n",
    "\n",
    "def augmented_lagrangian(f, con, x, delta_f, rho=1, grad_tol=1E-05, delta_f_tol=1E-05, delta_x_tol=1E-05, max_iter=100, full_output=False):\n",
    "    '''\n",
    "    Optimizes the constrained problem using the augmented Lagrangian method\n",
    "    Args:\n",
    "        f: objective function\n",
    "        con: constraint function\n",
    "        x: initial value of vector of variables\n",
    "        delta_f: gradient of the objective function\n",
    "        rho: augmented Lagrangian penalty parameter\n",
    "        grad_tol: gradient tolerance\n",
    "        delta_f_tol: objective function update tolerance\n",
    "        delta_x_tol: design variable update tolerance\n",
    "        max_iter: maximum number of iterations\n",
    "        full_output: whether to return the full output or not\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "        f_val: objective function value\n",
    "        exit_flag: exit flag\n",
    "        iter: number of iterations\n",
    "    '''\n",
    "    \n",
    "    g, h = con(x) # Calculate the initial constraint values\n",
    "    mu0 = np.zeros(g.shape[0]).T # Initialize the inequality Lagrangian multipliers\n",
    "    lambda_0 = np.zeros(h.shape[0]).T # Initialize the equality Lagrangian multipliers\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "\n",
    "    while not convergence:\n",
    "        x_new = quasi_newton(lambda x: calc_augmented_lagrangian(f, con, x, lambda_0, mu0, rho), \\\n",
    "            x, delta_f, delta_f_tol, delta_x_tol, max_iter=100, full_output=False) # Optimize the augmented Lagrangian function\n",
    "        g, h = con(x_new) # Calculate the constraint values\n",
    "        mu_new = mu0 + rho*g # Calculate the new inequality Lagrangian multipliers\n",
    "        # Check for negative inequality multipliers:\n",
    "        for i in range(len(mu_new)):\n",
    "            if mu_new[i] < 0:\n",
    "                mu_new[i] = 0\n",
    "        lambda_new = lambda_0 + rho*h # Calculate the new equality Lagrangian multipliers\n",
    "        delta_L_new = delta_f(lambda x: calc_lagrangian(f, con, x, lambda_new, mu_new), x_new) # Calculate the new Lagrangian gradient\n",
    "        iter += 1 # Increment the iteration counter\n",
    "        \n",
    "        # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.linalg.norm(delta_f(f, x_new)) <= grad_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.linalg.norm(x_new - x) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        elif np.linalg.norm(f(x_new) - f(x)) <= delta_f_tol:    # Check if objective function update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 3\n",
    "        \n",
    "        x = x_new # Update x\n",
    "        mu0 = mu_new # Update the Lagrangian multipliers\n",
    "        lambda_0 = lambda_new # Update the Lagrangian multipliers\n",
    "\n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test objective function\n",
    "def f(x):\n",
    "    return -(x[0] + x[1])\n",
    "\n",
    "# Test constraint function\n",
    "def con(x):\n",
    "    g = np.array([x[0]**2 + 2*x[1]**2 - 2])\n",
    "    h = np.array([])\n",
    "    return g, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 0]], dtype=float).T # Initialize the design variable\n",
    "\n",
    "f_unc = constrained_to_unconstrained(f, con, x) # Convert the constrained function to an unconstrained function\n",
    "\n",
    "print(augmented_lagrangian(f, con, x, forward_diff, full_output=True)) # Optimize the constrained problem using the augmented Lagrangian method\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82a59c0bad32f961f86610a9c6f7d5bee0daa017c0134de324e8d46a81872590"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
