{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries for optimization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(A, b):\n",
    "    '''\n",
    "    Solves the least squares problem Ax = b\n",
    "    Args:\n",
    "        A: co-efficients matrix\n",
    "        b: constants vector\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "    '''\n",
    "\n",
    "    return np.linalg.inv(A.T @ A) @ A.T @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite difference methods:\n",
    "def forward_diff(f, x, h=1E-08):\n",
    "    '''\n",
    "    Solves the forward difference problem\n",
    "    Args:\n",
    "        f: function\n",
    "        x: vector of variables\n",
    "        h: step size\n",
    "    Returns:\n",
    "        delta_f: solution vector\n",
    "    '''\n",
    "\n",
    "    delta_f = np.zeros(x.shape) # Initialize delta_f\n",
    "    for i in range(x.shape[0]):\n",
    "        x_forw = np.array(x) # Make a copy of x\n",
    "        x_forw[i] += h   # Increment x_forw[i] by h\n",
    "        delta_f[i] = (f(x_forw) - f(x)) / h # Calculate the forward difference\n",
    "\n",
    "    return delta_f\n",
    "\n",
    "\n",
    "def central_diff(f, x, h=1E-08):\n",
    "    '''\n",
    "    Solves the central difference problem\n",
    "    Args:\n",
    "        f: function\n",
    "        x: vector of variables\n",
    "        h: step size\n",
    "    Returns:\n",
    "        delta_f: solution vector\n",
    "    '''\n",
    "\n",
    "    delta_f = np.zeros(x.shape) # Initialize delta_f\n",
    "    for i in range(x.shape[0]):\n",
    "        x_forw = np.array(x) # Make a copy of x\n",
    "        x_back = np.array(x) # Make a copy of x\n",
    "        x_forw[i] += h   # Increment x_forw[i] by h\n",
    "        x_back[i] -= h   # Decrement x_back[i] by h\n",
    "        delta_f [i]= (f(x_forw) - f(x_back)) / (2*h) # Calculate the central difference\n",
    "\n",
    "    return delta_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting constrained optimization problem to unconstrained optimization problem:\n",
    "# Using penalty method:\n",
    "def constrained_to_unconstrained(f, con, x, p=100):\n",
    "    '''\n",
    "    Converts a constrained optimization problem to an unconstrained optimization problem\n",
    "    Args:\n",
    "        f: function\n",
    "        con: constraint function\n",
    "        x: vector of variables\n",
    "        p: penalty parameter\n",
    "    Returns:\n",
    "        f_uncon: unconstrained function\n",
    "    '''\n",
    "    \n",
    "    # Objective function\n",
    "    def f_unconstrained(x):\n",
    "        g, h = con(x) # Calculate the constraint function\n",
    "        con_sum = 0 # Initialize the constraint sum\n",
    "        for i in range(g.shape[0]):\n",
    "            con_sum += max(0, g[i])**2 # Add the constraint sum\n",
    "        for i in range(h.shape[0]):\n",
    "            con_sum += h[i]**2  # Add the constraint sum\n",
    "        return f(x) + p*con_sum # Return the unconstrained objective function\n",
    "        \n",
    "    return f_unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unconstrained optimization:\n",
    "# 1st order methods:\n",
    "\n",
    "def steepest_descent(f, x, delta_f, grad_tol=1E-05, delta_f_tol=1E-05, delta_x_tol=1E-05, max_iter=100, full_output=False):\n",
    "    '''\n",
    "    Optimizes the unconstrained problem using the steepest descent method\n",
    "    Args:\n",
    "        f: objective function\n",
    "        x: initial value of vector of variables\n",
    "        delta_f: gradient of the objective function\n",
    "        grad_tol: gradient tolerance\n",
    "        delta_f_tol: objective function update tolerance\n",
    "        delta_x_tol: design variable update tolerance\n",
    "        max_iter: maximum number of iterations\n",
    "        full_output: whether to return the full output or not\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "        f_val: objective function value\n",
    "        exit_flag: exit flag\n",
    "        iter: number of iterations\n",
    "    '''\n",
    "\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "    while not convergence:\n",
    "        s = -delta_f(f, x) # Calculate the search direction\n",
    "        s = s / np.linalg.norm(s) # Normalize the search direction\n",
    "        alpha = opt.fminbound(lambda alpha: f(x + alpha*s), 0, 1) # Calculate the step size\n",
    "        x_new = x + alpha*s # Calculate the new x\n",
    "        iter += 1 # Increment the iteration counter\n",
    "        \n",
    "        # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.linalg.norm(delta_f(f, x_new)) <= grad_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.linalg.norm(x_new - x) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        elif np.linalg.norm(f(x_new) - f(x)) <= delta_f_tol:    # Check if objective function update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 3\n",
    "            \n",
    "        x = x_new # Update x\n",
    "\n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    else:\n",
    "        return x_new\n",
    "\n",
    "\n",
    "def conjugate_gradient(f, x, delta_f, grad_tol=1E-05, delta_f_tol=1E-05, delta_x_tol=1E-05, max_iter=100, full_output=False):\n",
    "    '''\n",
    "    Optimizes the unconstrained problem using the conjugate gradient method\n",
    "    Args:\n",
    "        f: objective function\n",
    "        x: initial value of vector of variables\n",
    "        delta_f: gradient of the objective function\n",
    "        grad_tol: gradient tolerance\n",
    "        delta_f_tol: objective function update tolerance\n",
    "        delta_x_tol: design variable update tolerance\n",
    "        max_iter: maximum number of iterations\n",
    "        full_output: whether to return the full output or not\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "        f_val: objective function value\n",
    "        exit_flag: exit flag\n",
    "        iter: number of iterations\n",
    "    '''\n",
    "\n",
    "    s = -delta_f(f, x) # Calculate the initial search direction\n",
    "    s = s / np.linalg.norm(s) # Normalize the initial search direction\n",
    "    alpha = opt.fminbound(lambda alpha: f(x + alpha*s), 0, 1) # Calculate the initial step size\n",
    "    x_new = x + alpha*s # Calculate the initial x update\n",
    "\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "    while not convergence:\n",
    "        if iter % x.shape[0] == 0:  # Check if the iteration counter is a multiple of the number of design variables\n",
    "            s = -delta_f(f, x_new) # Calculate the search direction\n",
    "        else:\n",
    "            s = -delta_f(f, x_new) + (np.linalg.norm(delta_f(f, x_new))**2 / np.linalg.norm(delta_f(f, x))**2) * s # Calculate the search direction\n",
    "\n",
    "        s = s / np.linalg.norm(s) # Normalize the search direction\n",
    "        alpha = opt.fminbound(lambda alpha: f(x_new + alpha*s), 0, 1) # Calculate the step size\n",
    "        x = x_new # Update x\n",
    "        x_new = x + alpha*s # Calculate the new x\n",
    "        iter += 1 # Increment the iteration counter\n",
    "\n",
    "        # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.linalg.norm(delta_f(f, x_new)) <= grad_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.linalg.norm(x_new - x) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        elif np.linalg.norm(f(x_new) - f(x)) <= delta_f_tol:    # Check if objective function update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 3\n",
    "        \n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    else:\n",
    "        return x_new\n",
    "\n",
    "\n",
    "# Hessian update rules:\n",
    "def BFGS_update(B, delta_x, delta_delta):\n",
    "    '''\n",
    "    Calculates the BFGS update matrix\n",
    "    Args:\n",
    "        B: initial Hessian estimate\n",
    "        delta_x: design variable update\n",
    "        delta_delta: objective function update\n",
    "    Returns:\n",
    "        deta_B: update to the Hessian estimate\n",
    "    '''\n",
    "\n",
    "    return (1 + delta_delta.T@B@delta_delta / (delta_delta.T@delta_x)) * (delta_x@delta_x.T) / (delta_x.T@delta_delta) - \\\n",
    "        (delta_x@(delta_delta.T@B) + (delta_delta.T@B).T@delta_x.T) / (delta_x.T@delta_delta)\n",
    "\n",
    "def DFP_update(B, delta_x, delta_delta):\n",
    "    '''\n",
    "    Calculates the DFP update matrix\n",
    "    Args:\n",
    "        B: initial Hessian estimate\n",
    "        delta_x: design variable update\n",
    "        delta_delta: objective function update\n",
    "    Returns:\n",
    "        deta_B: update to the Hessian estimate\n",
    "    '''\n",
    "\n",
    "    return delta_x@delta_x.T / (delta_x.T@delta_delta) - \\\n",
    "        (B@delta_delta)@(B@delta_delta).T / (delta_delta.T@B@delta_delta)\n",
    "\n",
    "\n",
    "# Quasi-Newton method:\n",
    "def quasi_newton(f, x, delta_f, grad_tol=1E-05, delta_f_tol=1E-05, delta_x_tol=1E-05, max_iter=100, full_output=False, update_rule=BFGS_update):\n",
    "    '''\n",
    "    Optimizes the unconstrained problem using the quasi-Newton method\n",
    "    Args:\n",
    "        f: objective function\n",
    "        x: initial value of vector of variables\n",
    "        delta_f: gradient of the objective function\n",
    "        grad_tol: gradient tolerance\n",
    "        delta_f_tol: objective function update tolerance\n",
    "        delta_x_tol: design variable update tolerance\n",
    "        max_iter: maximum number of iterations\n",
    "        full_output: whether to return the full output or not\n",
    "        update_rule: update rule for the Hessian estimate\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "        f_val: objective function value\n",
    "        exit_flag: exit flag\n",
    "        iter: number of iterations\n",
    "    '''\n",
    "\n",
    "    B = np.eye(x.shape[0]) # Initialize the Hessian approximation\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "\n",
    "    while not convergence:\n",
    "        s = -B@delta_f(f, x)   # Calculate the search direction\n",
    "        s = s / np.linalg.norm(s) # Normalize the search direction\n",
    "        alpha = opt.fminbound(lambda alpha: f(x + alpha*s), 0, 1) # Calculate the step size\n",
    "        x_new = x + alpha*s # Calculate the new x\n",
    "        iter += 1 # Increment the iteration counter\n",
    "\n",
    "         # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.linalg.norm(delta_f(f, x_new)) <= grad_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.linalg.norm(x_new - x) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        elif np.linalg.norm(f(x_new) - f(x)) <= delta_f_tol:    # Check if objective function update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 3\n",
    "\n",
    "        B = B + update_rule(B, x_new - x, delta_f(f, x_new) - delta_f(f, x)) # Update the Hessian approximation\n",
    "        x = x_new # Update x\n",
    "\n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    else:\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained optimization methods:\n",
    "\n",
    "def calc_lagrangian(f, con, x, lambda_, mu):\n",
    "    '''\n",
    "    Calculates the lagrangian of the function\n",
    "    Args:\n",
    "        f: objective function\n",
    "        con: constraint function\n",
    "        x: vector of variables\n",
    "        lambda_: Lagrange equality multipliers\n",
    "        mu: Lagrange inequality multipliers\n",
    "    Returns:\n",
    "        lagrangian: lagrangian of the function\n",
    "    '''\n",
    "    g, h = con(x)   # Calculate the inequality and equality constraints\n",
    "    \n",
    "    return f(x) + lambda_@h + mu@g   # Calculate the lagrangian\n",
    "\n",
    "\n",
    "# Augmented Lagrangian method:\n",
    "def calc_augmented_lagrangian(f, con, x, lambda_, mu, rho):\n",
    "    '''\n",
    "    Calculates the augmented Lagrangian of the function\n",
    "    Args:\n",
    "        f: objective function\n",
    "        con: constraint function\n",
    "        x: vector of variables\n",
    "        lambda_: Lagrange equality multipliers\n",
    "        mu: Lagrange inequality multipliers\n",
    "        rho: augmented Lagrangian penalty parameter\n",
    "    Returns:\n",
    "        augmented_lagrangian: augmented Lagrangian of the function\n",
    "    '''\n",
    "\n",
    "    g, h = con(x) # Calculate the inequality and equality constraints\n",
    "    con_sum = 0\n",
    "    for i in range(g.shape[0]):\n",
    "        con_sum += max(0, g[i])**2\n",
    "    for i in range(h.shape[0]):\n",
    "        con_sum += h[i]**2\n",
    "    \n",
    "    return calc_lagrangian(f, con, x, lambda_, mu) +  rho * con_sum # Calculate the augmented Lagrangian function\n",
    "\n",
    "\n",
    "def augmented_lagrangian(f, con, x, delta_f, rho=1, grad_tol=1E-05, delta_f_tol=1E-05, delta_x_tol=1E-05, max_iter=100, full_output=False):\n",
    "    '''\n",
    "    Optimizes the constrained problem using the augmented Lagrangian method\n",
    "    Args:\n",
    "        f: objective function\n",
    "        con: constraint function\n",
    "        x: initial value of vector of variables\n",
    "        delta_f: gradient of the objective function\n",
    "        rho: augmented Lagrangian penalty parameter\n",
    "        grad_tol: gradient tolerance\n",
    "        delta_f_tol: objective function update tolerance\n",
    "        delta_x_tol: design variable update tolerance\n",
    "        max_iter: maximum number of iterations\n",
    "        full_output: whether to return the full output or not\n",
    "    Returns:\n",
    "        x: solution vector\n",
    "        f_val: objective function value\n",
    "        exit_flag: exit flag\n",
    "        iter: number of iterations\n",
    "    '''\n",
    "    \n",
    "    g, h = con(x) # Calculate the initial constraint values\n",
    "    mu0 = np.zeros(g.shape[0]).T # Initialize the inequality Lagrangian multipliers\n",
    "    lambda_0 = np.zeros(h.shape[0]).T # Initialize the equality Lagrangian multipliers\n",
    "    convergence = False\n",
    "    iter = 0\n",
    "\n",
    "    while not convergence:\n",
    "        x_new = quasi_newton(lambda x: calc_augmented_lagrangian(f, con, x, lambda_0, mu0, rho), \\\n",
    "            x, delta_f, delta_f_tol, delta_x_tol, max_iter=100, full_output=False) # Optimize the augmented Lagrangian function\n",
    "        g, h = con(x_new) # Calculate the constraint values\n",
    "        mu_new = mu0 + rho*g # Calculate the new inequality Lagrangian multipliers\n",
    "        # Check for negative inequality multipliers:\n",
    "        for i in range(len(mu_new)):\n",
    "            if mu_new[i] < 0:\n",
    "                mu_new[i] = 0\n",
    "        lambda_new = lambda_0 + rho*h # Calculate the new equality Lagrangian multipliers\n",
    "        delta_L_new = delta_f(lambda x: calc_lagrangian(f, con, x, lambda_new, mu_new), x_new) # Calculate the new Lagrangian gradient\n",
    "        iter += 1 # Increment the iteration counter\n",
    "        \n",
    "        # Check for convergence:\n",
    "        if iter >= max_iter:    # Check if maximum number of iterations is reached\n",
    "            convergence = True\n",
    "            exit_flag = 0\n",
    "        elif np.linalg.norm(delta_f(f, x_new)) <= grad_tol:    # Check if gradient is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 1\n",
    "        elif np.linalg.norm(x_new - x) <= delta_x_tol:  # Check if design variable update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 2\n",
    "        elif np.linalg.norm(f(x_new) - f(x)) <= delta_f_tol:    # Check if objective function update is within tolerance\n",
    "            convergence = True\n",
    "            exit_flag = 3\n",
    "        \n",
    "        x = x_new # Update x\n",
    "        mu0 = mu_new # Update the Lagrangian multipliers\n",
    "        lambda_0 = lambda_new # Update the Lagrangian multipliers\n",
    "\n",
    "    if full_output:\n",
    "        return x_new, f(x_new), exit_flag, iter\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test objective function\n",
    "def f(x):\n",
    "    return -(x[0] + x[1])\n",
    "\n",
    "# Test constraint function\n",
    "def con(x):\n",
    "    g = np.array([x[0]**2 + 2*x[1]**2 - 2])\n",
    "    h = np.array([])\n",
    "    return g, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 0]], dtype=float).T # Initialize the design variable\n",
    "\n",
    "f_unc = constrained_to_unconstrained(f, con, x) # Convert the constrained function to an unconstrained function\n",
    "\n",
    "print(augmented_lagrangian(f, con, x, forward_diff, full_output=True)) # Optimize the constrained problem using the augmented Lagrangian method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and descaling functions:\n",
    "def scale(x_raw, x_ref):\n",
    "    '''\n",
    "    Scales the design variable\n",
    "    Args:\n",
    "        x_raw: raw design variable\n",
    "        x_ref: reference design variable\n",
    "    Returns:\n",
    "        x_scaled: scaled design variable\n",
    "    '''\n",
    "    return np.divide(x_raw, x_ref)\n",
    "\n",
    "def descale(x_scaled, x_ref):\n",
    "    '''\n",
    "    Descales the design variable\n",
    "    Args:\n",
    "        x_scaled: scaled design variable\n",
    "        x_ref: reference design variable\n",
    "    Returns:\n",
    "        x_raw: raw design variable\n",
    "    '''\n",
    "    return np.multiply(x_scaled, x_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pressure_ratio_from_area_ratio(area_ratio):\n",
    "    '''\n",
    "    Calculates the pressure ratio from the area ratio\n",
    "    Args:\n",
    "        area_ratio: area ratio\n",
    "    Returns:\n",
    "        pressure_ratio: pressure ratio\n",
    "    '''\n",
    "    import constants as c\n",
    "\n",
    "    # Temperory calculation variables:\n",
    "    j = 2 * c.gamma / (c.gamma - 1)\n",
    "    k = 2 / c.gamma\n",
    "    l = (c.gamma - 1) / c.gamma\n",
    "\n",
    "    pressure_ratio_0 = 0.017 # Initial guess for pressure ratio\n",
    "    # Calculate the pressure ratio using fsolve:\n",
    "    pressure_ratio = opt.fsolve(lambda pressure_ratio: area_ratio - c.Gamma / np.sqrt(j * pressure_ratio**k * (1 - pressure_ratio**l)), pressure_ratio_0, xtol=1E-06)\n",
    "    return pressure_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the objective function:\n",
    "def objective(x_scaled):\n",
    "    '''\n",
    "    Calculates the objective function value\n",
    "    Args:\n",
    "        x_scaled: scaled design variable\n",
    "    Returns:\n",
    "        f_val: dry mass of rocket considering tank, chamber, injector plate and nozzle masses [kg]\n",
    "    '''\n",
    "    # Importing constants:\n",
    "    import constants as c\n",
    "\n",
    "    # Descaling the design variables:\n",
    "    P_c, A_t, A_e = descale(x_scaled, c.x_ref)\n",
    "\n",
    "    # Calculating fuel and oxidizer tank masses:\n",
    "    # cylindrical section thickness [m]\n",
    "    t_cyl = c.f * P_c * c.R_tank / c.sigma_tank\n",
    "    t_sph = t_cyl / 2    # spherical section thickness [m]\n",
    "    mass_UDMH_tank = (2*np.pi*t_cyl*c.R_tank*c.L_UDMH_tank + 4*np.pi*t_sph *\n",
    "                        c.R_tank**2) * c.rho_tank    # mass of the UDMH tank [kg]\n",
    "    mass_N2O4_tank = (2*np.pi*t_cyl*c.R_tank*c.L_N2O4_tank + 4*np.pi*t_sph *\n",
    "                        c.R_tank**2) * c.rho_tank    # mass of the N2O4 tank [kg]\n",
    "\n",
    "    # Ideal rocket theory calculations:\n",
    "    mass_flow = c.Gamma * P_c * A_t / np.sqrt(c.R * c.T_c) # mass flow through the nozzle [kg/s]\n",
    "    A_c = (mass_flow * c.R * c.T_c) / \\\n",
    "        (0.3 * P_c * np.sqrt(c.gamma * c.R * c.T_c))    # cross sectional area of the combustion chamber (assuming M = 0.3 for flow exiting nozzle) [m^2]\n",
    "    R_c = np.sqrt(A_c / np.pi)    # radius of the combustion chamber [m]\n",
    "    V_c = np.pi * R_c**2 * c.L_c    # volume of the combustion chamber [m^3]\n",
    "    k_load = 1    # correction factor for high chamber pressures\n",
    "\n",
    "    # Calculating the chamber and nozzle masses:\n",
    "    mass_chamber = c.f * k_load * (2/(c.L_c/R_c) + 2) * c.rho/c.sigma * P_c * V_c    # mass of the combustion chamber [kg]\n",
    "    mass_injector = c.f * (c.rho/c.sigma) * (1.2 * A_c * R_c * np.sqrt(P_c * c.sigma))  # mass of the injector plate [kg]\n",
    "    mass_nozzle = c.f * (c.rho/c.sigma) * (A_t * ((A_e/A_t - 1)/np.sin(c.alpha)) * P_c * R_c)  # mass of the nozzle [kg]\n",
    "\n",
    "    return (mass_UDMH_tank + mass_N2O4_tank + mass_chamber + mass_injector + mass_nozzle) / c.mass_dry_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constraints:\n",
    "def constraints(x_scaled):\n",
    "    '''\n",
    "    Caclulates the constraint values\n",
    "    Args:\n",
    "        x_scaled: scaled design variable\n",
    "    Returns:\n",
    "        g: inequality constraint values\n",
    "        h: equality constraint values\n",
    "    '''\n",
    "\n",
    "    # Importing constants:\n",
    "    import constants as c\n",
    "    \n",
    "    # Descaling the design variables:\n",
    "    P_c, A_t, A_e = descale(x_scaled, c.x_ref)\n",
    "\n",
    "    mass_flow = c.Gamma * P_c * A_t / np.sqrt(c.R * c.T_c) # mass flow through the nozzle [kg/s]\n",
    "\n",
    "    # Temperory calculation variables:\n",
    "    j = 2 * c.gamma / (c.gamma - 1)\n",
    "    k = 2 / c.gamma\n",
    "    l = (c.gamma - 1) / c.gamma\n",
    "    \n",
    "    pressure_ratio = pressure_ratio_from_area_ratio(A_e/A_t) # nozzle pressure ratio\n",
    "    P_e = P_c * pressure_ratio  # pressure at the exit of the nozzle [Pa]\n",
    "\n",
    "    u_e = np.sqrt(j * c.R * c.T_c * (1 - pressure_ratio**l))    # velocity at the exit of the nozzle [m/s]\n",
    "    thrust = mass_flow * u_e + (P_e - c.P_a) * A_e    # thrust [N]\n",
    "    mass_wet = objective(x_scaled) * c.mass_dry_ref + c.mass_N2O4 + c.mass_UDMH    # wet mass of the rocket [kg]\n",
    "    thrust_factor = thrust / (mass_wet * c.g0)  # thrust factor\n",
    "    Isp = thrust / (mass_flow * c.g0)   # specific impulse [s]\n",
    "\n",
    "    g = np.array([1 - Isp/c.Isp_ref])   # inequality constraint values\n",
    "    h = np.array([1 - thrust_factor/c.thrust_factor_ref, 1 - mass_flow/c.mass_flow_ref])   # equality constraint values\n",
    "\n",
    "    return g, h\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82a59c0bad32f961f86610a9c6f7d5bee0daa017c0134de324e8d46a81872590"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
