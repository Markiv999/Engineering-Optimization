{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fsolve, fminbound, fmin_bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quasi-Newton unconstained optimization \n",
    "def quasi_newton_opt(x0, func, func_grad, max_iter=500, full_output=False):\n",
    "    # Function to perform a quasi-Newton optimization\n",
    "    # for an unconstrained problem with BFGS Hessian update\n",
    "\n",
    "    # x0: Inital point\n",
    "    # func_grad: Gradient of function\n",
    "    # func_line_search: Objective function with used for line search\n",
    "\n",
    "    B0 = np.identity(len(x0))    # initializing hessian\n",
    "    iter_no = 0\n",
    "    while True:\n",
    "        delta_f0 = func_grad(x0, func)    # calculating gradient\n",
    "        sq = np.dot(-B0, delta_f0)  # calculating search direction\n",
    "        alpha = fminbound(lambda alpha: func(x0 + alpha*sq), 0, 10000)  # line search along sq\n",
    "        x_new = x0 + alpha * sq # new point\n",
    "        delta_f_new = func_grad(x_new, func)  # gradient at new point \n",
    "\n",
    "        # Checking for stationary point or max iterations:\n",
    "        if max(abs(delta_f_new)) < 1E-07 or iter_no > max_iter:\n",
    "            break\n",
    "\n",
    "        delta_x = x_new - x0    # change in x\n",
    "        delta_delta = delta_f_new - delta_f0    # change in gradient\n",
    "        delta_B = BFGS_update(B0, delta_x, delta_delta) # BFGS hessian update\n",
    "\n",
    "        # Updating values for next iteration\n",
    "        B_new = B0 + delta_B\n",
    "        B0 = B_new\n",
    "        x0 = x_new\n",
    "        delta_f0 = delta_f_new\n",
    "        iter_no = iter_no + 1\n",
    "        \n",
    "    if full_output:\n",
    "        return [x_new, delta_f_new, B_new, iter_no]\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    # Objective function definition\n",
    "\n",
    "    # x: Evaluation point\n",
    "    f = 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2 + 100*(x[2] - x[1]**2)**2 + (1 - x[1])**2\n",
    "    return f\n",
    "\n",
    "def test_func(x):\n",
    "    return np.sin(x[0] + x[1]) + (x[0] - x[1])**2 - 1.5*x[0] + 2.5*x[1] + 1\n",
    "\n",
    "def func_grad(x, func):\n",
    "    # Function to calculate gradient \n",
    "    # of objective function using forward FD\n",
    "\n",
    "    # x: Evaluation point\n",
    "\n",
    "    step = 1E-8 # step size for FD\n",
    "    delta_f = np.zeros((len(x), 1)) # initializing gradient\n",
    "    for i in range(len(x)):\n",
    "        x_temp = np.array(x, dtype=float)\n",
    "        x_temp[i] = x[i] + step\n",
    "        delta_f[i] = (func(x_temp) - func(x)) / step    # calculating gradient using forward FD\n",
    "    return delta_f\n",
    "\n",
    "# def func_grad(x):\n",
    "#     # Function to calculate gradient \n",
    "#     # of objective function using forward FD\n",
    "\n",
    "#     # x: Evaluation point\n",
    "\n",
    "#     step = 1E-8 # step size for FD\n",
    "#     delta_f = np.zeros((2, 1))\n",
    "#     delta_f[0] = (func(np.array([x[0]+step, x[1]])) - func(x)) / step\n",
    "#     delta_f[1] = (func(np.array([x[0], x[1]+step])) - func(x)) / step\n",
    "#     return delta_f\n",
    "\n",
    "\n",
    "\n",
    "def BFGS_update(B, delta_x, delta_delta):\n",
    "    # Function to update to hessian\n",
    "    # using BFGS method\n",
    "\n",
    "    # B: initial hessian\n",
    "    # delta_x: change in evaluation point\n",
    "    # delta_delta: change in gradient\n",
    "    \n",
    "    delta_x_T = delta_x.transpose()\n",
    "    delta_delta_T = delta_delta.transpose()\n",
    "    delta_B = (1 + (np.dot(np.dot(delta_delta_T, B), delta_delta))/(np.dot(delta_x_T, delta_delta))) * \\\n",
    "    (np.dot(delta_x, delta_x_T)/np.dot(delta_x_T, delta_delta)) - \\\n",
    "        (np.dot(delta_x, np.dot(delta_delta_T, B)) + np.dot(np.dot(delta_delta_T, B).transpose(), delta_x_T)) / \\\n",
    "            (np.dot(delta_x_T, delta_delta))\n",
    "    \n",
    "    return delta_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([[0, 0, 0]]).transpose()\n",
    "[x, delta_f, B, iter_no] = quasi_newton_opt(x0, func, func_grad, max_iter=1000, full_output=True)\n",
    "print(f'Optima: \\n{x}\\nGradient at optima: \\n{delta_f}\\nHessian inverse at optima: \\n{B}')\n",
    "print(f'Iterations: {iter_no}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0, 0, 0], dtype=float)\n",
    "x = fmin_bfgs(func, x0)\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82a59c0bad32f961f86610a9c6f7d5bee0daa017c0134de324e8d46a81872590"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
